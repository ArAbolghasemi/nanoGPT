{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "import torch\n",
    "import tiktoken\n",
    "from model import GPTConfig, GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alireza\\AppData\\Local\\Temp\\ipykernel_10812\\4173216363.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "out_dir = r'C:\\Users\\alireza\\Documents\\git\\nanoGPT\\out-moSeq-syll_e1'\n",
    "ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
    "device = 'cuda'\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model', 'optimizer', 'model_args', 'iter_num', 'best_val_loss', 'config'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alireza\\AppData\\Local\\Temp\\ipykernel_9300\\3443039349.py:43: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using fast attention with Flash Attention CUDA kernels\n",
      "using fast attention with Flash Attention CUDA kernels\n",
      "using fast attention with Flash Attention CUDA kernels\n",
      "using fast attention with Flash Attention CUDA kernels\n",
      "using fast attention with Flash Attention CUDA kernels\n",
      "using fast attention with Flash Attention CUDA kernels\n",
      "number of parameters: 10.64M\n",
      "Loading meta from data\\moSeq_syllabus_small\\meta.pkl...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nwith torch.no_grad():\\n    with ctx:\\n        for k in range(num_samples):\\n            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\\n            if print_samples:\\n                print(decode(y[0].tolist()))\\n                print(\\'---------------\\')\\n            if save_samples:\\n                if data_type == \\'text\\':\\n                    # save the samples to a text file\\n                    file_name = f\"{sample_file_name}_{k}.txt\"\\n                    file_path = os.path.join(out_dir, file_name)\\n                    # rewrite the file if it already exists\\n                    with open(file_path, \\'w\\', encoding=\\'utf-8\\') as f:\\n                        f.write(decode(y[0].tolist()))\\n                elif data_type == \\'numpy_array\\':\\n                    # save the samples to a numpy file\\n                    file_name = f\"{sample_file_name}_{k}.npy\"\\n                    file_path = os.path.join(out_dir, file_name)\\n                    # rewrite the file if it already exists\\n                    np.save(file_path, y[0].cpu().numpy())\\n                else:\\n                    raise ValueError(f\"Unknown data_type: {data_type}\")\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Sample from a trained model\n",
    "\"\"\"\n",
    "import os\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "import torch\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "from model import GPTConfig, GPT\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "init_from = 'resume' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\n",
    "out_dir = r'C:\\Users\\alireza\\Documents\\git\\nanoGPT\\out-moSeq-syll_e1' # ignored if init_from is not 'resume'\n",
    "data_type = 'numpy_array' # or 'numpy_array' for the moSeq dataset\n",
    "start = \"\\n\"#r'FILE:C:\\Users\\alireza\\Documents\\git\\nanoGPT\\data\\moSeq_syllabus_small\\val_data_sample.npy' #\"\\n\" or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\" or a numpy array \"FILE:prompt.npy\"\n",
    "num_samples = 1 # number of samples to draw\n",
    "max_new_tokens = 1000 # number of tokens generated in each sample\n",
    "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "seed = 1337\n",
    "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
    "sample_file_name = 'samples' # where to save the samples\n",
    "print_samples = True # print the samples to the console\n",
    "save_samples = True # save the samples to a file\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
    "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
    "#exec(open('configurator.py').read()) # overrides from command line or config file\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "# model\n",
    "if init_from == 'resume':\n",
    "    # init from a model saved in a specific directory\n",
    "    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "    model = GPT(gptconf)\n",
    "    state_dict = checkpoint['model']\n",
    "    unwanted_prefix = '_orig_mod.'\n",
    "    for k,v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "    model.load_state_dict(state_dict)\n",
    "elif init_from.startswith('gpt2'):\n",
    "    # init from a given GPT-2 model\n",
    "    model = GPT.from_pretrained(init_from, dict(dropout=0.0))\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "if compile:\n",
    "    model = torch.compile(model) # requires PyTorch 2.0 (optional)\n",
    "\n",
    "# look for the meta pickle in case it is available in the dataset folder\n",
    "load_meta = False\n",
    "if init_from == 'resume' and 'config' in checkpoint and 'dataset' in checkpoint['config']: # older checkpoints might not have these...\n",
    "    meta_path = os.path.join('data', checkpoint['config']['dataset'], 'meta.pkl')\n",
    "    load_meta = os.path.exists(meta_path)\n",
    "if load_meta:\n",
    "    print(f\"Loading meta from {meta_path}...\")\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    # TODO want to make this more general to arbitrary encoder/decoder schemes\n",
    "    stoi, itos = meta['stoi'], meta['itos']\n",
    "    if data_type == 'text':\n",
    "        encode = lambda s: [stoi[c] for c in s]\n",
    "        decode = lambda l: ''.join([itos[i] for i in l])\n",
    "    elif data_type == 'numpy_array':\n",
    "        encode = lambda s: [stoi[c] for c in s]\n",
    "        decode = lambda l: np.array([itos[i] for i in l])\n",
    "    else:\n",
    "        # TODO: you really need abstract this and make it more general - i.e., make it independent of the \n",
    "        # data_type and to the encoder/decoder then transter it the the data_type\n",
    "        raise ValueError(f\"Unknown data_type: {data_type}\")\n",
    "else:\n",
    "    # ok let's assume gpt-2 encodings by default\n",
    "    print(\"No meta.pkl found, assuming GPT-2 encodings...\")\n",
    "    enc = tiktoken.get_encoding(\"gpt2\")\n",
    "    encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
    "    decode = lambda l: enc.decode(l)\n",
    "\n",
    "# if the data is a natrual language text, so the data is a string \n",
    "if data_type == 'text':\n",
    "    # encode the beginning of the prompt\n",
    "    if start.startswith('FILE:'):\n",
    "        with open(start[5:], 'r', encoding='utf-8') as f:\n",
    "            start = f.read()\n",
    "    start_ids = encode(start)\n",
    "    x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "\n",
    "elif data_type == 'numpy_array':\n",
    "    if start.startswith('FILE:'):\n",
    "        start = np.load(start[5:], allow_pickle=True)\n",
    "    else:\n",
    "        start = decode([1])\n",
    "    start_ids = encode(start)\n",
    "    x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "else:\n",
    "    raise ValueError(f\"Unknown data_type: {data_type}\")\n",
    "    # run generation\n",
    "\"\"\"\n",
    "with torch.no_grad():\n",
    "    with ctx:\n",
    "        for k in range(num_samples):\n",
    "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "            if print_samples:\n",
    "                print(decode(y[0].tolist()))\n",
    "                print('---------------')\n",
    "            if save_samples:\n",
    "                if data_type == 'text':\n",
    "                    # save the samples to a text file\n",
    "                    file_name = f\"{sample_file_name}_{k}.txt\"\n",
    "                    file_path = os.path.join(out_dir, file_name)\n",
    "                    # rewrite the file if it already exists\n",
    "                    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                        f.write(decode(y[0].tolist()))\n",
    "                elif data_type == 'numpy_array':\n",
    "                    # save the samples to a numpy file\n",
    "                    file_name = f\"{sample_file_name}_{k}.npy\"\n",
    "                    file_path = os.path.join(out_dir, file_name)\n",
    "                    # rewrite the file if it already exists\n",
    "                    np.save(file_path, y[0].cpu().numpy())\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown data_type: {data_type}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1]], device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MSBM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
